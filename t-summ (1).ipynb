{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Imports\nimport pandas as pd\nimport numpy as np\nfrom datasets import load_dataset\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom tqdm import tqdm\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T21:02:52.659242Z","iopub.execute_input":"2025-05-24T21:02:52.659526Z","iopub.status.idle":"2025-05-24T21:03:05.215662Z","shell.execute_reply.started":"2025-05-24T21:02:52.659503Z","shell.execute_reply":"2025-05-24T21:03:05.215119Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Hugging Face login\nfrom huggingface_hub import login\nlogin(\"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\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T21:03:05.216826Z","iopub.execute_input":"2025-05-24T21:03:05.217280Z","iopub.status.idle":"2025-05-24T21:03:05.393529Z","shell.execute_reply.started":"2025-05-24T21:03:05.217260Z","shell.execute_reply":"2025-05-24T21:03:05.393044Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\nbase_model_name = \"meta-llama/Llama-2-7b-hf\"\npeft_model_name = \"FinGPT/fingpt-mt_llama2-7b_lora\"\n\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token  # <--- ADD THIS LINE\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\nmodel = PeftModel.from_pretrained(base_model, peft_model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T21:03:05.394139Z","iopub.execute_input":"2025-05-24T21:03:05.394382Z","iopub.status.idle":"2025-05-24T21:05:42.016207Z","shell.execute_reply.started":"2025-05-24T21:03:05.394361Z","shell.execute_reply":"2025-05-24T21:05:42.015621Z"}},"outputs":[{"name":"stderr","text":"2025-05-24 21:03:07.449205: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748120587.661680      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748120587.719315      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a01ef76d63d486aadd3bf0e6bb60d62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7bc55ada2244f15b65a47db1f14e363"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6ca094645664fad9fde2261b0d5c22f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"babb130a090a464499bfbdd5359a11be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"286f70c4a45044b9af6570d65b4df90a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b91b64e7cac4db49c362aaeedb16ef6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"662a38828b734b2aa3b50310a428734f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6139172b14174fce98eb1617c1378c16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d36ed9bb2fa46a69fd343f400caad6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4a7f9aeb031449280b870a180c36cb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc2173306fa44a0380d5e6409af73fff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/460 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c131201d845645bdad4fc92fda7ddeaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.bin:   0%|          | 0.00/12.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c99112cec1346ea9e4654e049c53f13"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Cell 3: Load the ChanceFocus/flare-ectsum dataset (limit to 20)\ndataset = load_dataset(\"ChanceFocus/flare-ectsum\", split=\"test\")\n\nprint(dataset.features)  # Shows available columns\n\ntexts = dataset[\"text\"][:495]      # Take only the first 20 documents\nreferences = dataset[\"answer\"][:495]   # Take only the first 20 summaries","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T21:05:42.017578Z","iopub.execute_input":"2025-05-24T21:05:42.018120Z","iopub.status.idle":"2025-05-24T21:05:43.553056Z","shell.execute_reply.started":"2025-05-24T21:05:42.018101Z","shell.execute_reply":"2025-05-24T21:05:43.552489Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/488 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78ee98e98bd840cead12d2d0ac0149d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-8ef60b4155c29bac.parquet:   0%|          | 0.00/3.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d719fc18385400abfd5a9fd3f4d2987"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/495 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2590734b97d044838d00d7833e60f7b5"}},"metadata":{}},{"name":"stdout","text":"{'id': Value(dtype='string', id=None), 'query': Value(dtype='string', id=None), 'answer': Value(dtype='string', id=None), 'label': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'text': Value(dtype='string', id=None)}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 4: Generate Summaries with prompt engineering\n\ndef generate_summary(text, max_new_tokens=64):\n    # Add your prompt here\n    prompt = f\"Summarize the following text in 1-2 sentences:\\n\\n{text}\\n\\nSummary:\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n    summary_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, num_beams=4)\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary\n    \ngenerated_summaries = []\nfor text in tqdm(texts, desc=\"Generating summaries\"):\n    generated_summaries.append(generate_summary(text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T21:05:43.553707Z","iopub.execute_input":"2025-05-24T21:05:43.553906Z","iopub.status.idle":"2025-05-24T21:30:03.126773Z","shell.execute_reply.started":"2025-05-24T21:05:43.553881Z","shell.execute_reply":"2025-05-24T21:30:03.126131Z"}},"outputs":[{"name":"stderr","text":"Generating summaries:   0%|          | 0/495 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\nGenerating summaries: 100%|██████████| 495/495 [24:19<00:00,  2.95s/it]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install rouge_score --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T21:30:03.132567Z","iopub.execute_input":"2025-05-24T21:30:03.132830Z","iopub.status.idle":"2025-05-24T21:30:09.328233Z","shell.execute_reply.started":"2025-05-24T21:30:03.132805Z","shell.execute_reply":"2025-05-24T21:30:09.327480Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Install the evaluate library if not present\n!pip install evaluate --quiet\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import f1_score\nimport evaluate\n\n# Tokenize for F1 (token-based)\ndef tokenize(s):\n    return s.lower().split()\n\ntrue_token_lists = [tokenize(s) for s in references]\npred_token_lists = [tokenize(s) for s in generated_summaries]\n\nmlb = MultiLabelBinarizer()\nmlb.fit(true_token_lists + pred_token_lists)\n\ny_true_bin = mlb.transform(true_token_lists)\ny_pred_bin = mlb.transform(pred_token_lists)\n\nmicro = f1_score(y_true_bin, y_pred_bin, average='micro', zero_division=0) * 100\nmacro = f1_score(y_true_bin, y_pred_bin, average='macro', zero_division=0) * 100\nweighted = f1_score(y_true_bin, y_pred_bin, average='weighted', zero_division=0) * 100\n\nprint(\"Evaluation (in pts / ECTs):\")\nprint(f\"- Micro F1:    {micro:.2f} pts\")\nprint(f\"- Macro F1:    {macro:.2f} pts\")\nprint(f\"- Weighted F1: {weighted:.2f} pts\")\n\n# Compute ROUGE\nrouge = evaluate.load(\"rouge\")\nrouge_scores = rouge.compute(predictions=generated_summaries, references=references)\n\nprint(\"\\nROUGE (bullet-point results):\")\nprint(f\"- ROUGE-1: {rouge_scores['rouge1']*100:.2f} pts\")\nprint(f\"- ROUGE-2: {rouge_scores['rouge2']*100:.2f} pts\")\nprint(f\"- ROUGE-L: {rouge_scores['rougeL']*100:.2f} pts\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T21:30:09.329318Z","iopub.execute_input":"2025-05-24T21:30:09.329591Z","iopub.status.idle":"2025-05-24T21:30:24.969366Z","shell.execute_reply.started":"2025-05-24T21:30:09.329551Z","shell.execute_reply":"2025-05-24T21:30:24.968523Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mEvaluation (in pts / ECTs):\n- Micro F1:    0.14 pts\n- Macro F1:    0.00 pts\n- Weighted F1: 11.41 pts\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7d2e6a9ea444d6bbb92f776fa133930"}},"metadata":{}},{"name":"stdout","text":"\nROUGE (bullet-point results):\n- ROUGE-1: 1.86 pts\n- ROUGE-2: 0.02 pts\n- ROUGE-L: 1.83 pts\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}