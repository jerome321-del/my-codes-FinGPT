{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\nfrom tqdm import tqdm\n\n# --- 1. Settings ---\nFROM_REMOTE = True\n\n\nfrom huggingface_hub import login\nlogin(\"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\")\n\nbase_model = \"meta-llama/Llama-2-7b-hf\"\npeft_model = \"FinGPT/fingpt-mt_llama2-7b_lora\" if FROM_REMOTE else \"finetuned_models/MT-llama2-linear_202309210126\"\n\n# --- 2. Load Model and Tokenizer (Official Demo Style) ---\ndef load_model(base_model, peft_model, from_remote=False):\n    # Use huggingface string if remote, else path to local\n    model_name = base_model if from_remote else base_model  # Here base_model can be a path if local\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name, trust_remote_code=True, device_map=\"auto\"\n    )\n    model.model_parallel = True\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    tokenizer.padding_side = \"left\"\n    if base_model == 'qwen':\n        tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids('<|endoftext|>')\n        tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('<|extra_0|>')\n    if not tokenizer.pad_token or tokenizer.pad_token_id == tokenizer.eos_token_id:\n        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n        model.resize_token_embeddings(len(tokenizer))\n\n    model = PeftModel.from_pretrained(model, peft_model)\n    model = model.eval()\n    return model, tokenizer\n\nmodel, tokenizer = load_model(base_model, peft_model, FROM_REMOTE)\n\n# # --- 3. Define NER Inference Function Using Official Demo Prompt ---\n# def fingpt_ner(model, tokenizer, input_text):\n#     instruction = \"Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.\"\n#     prompt = f\"Instruction: {instruction}\\nInput: {input_text}\\nAnswer: \"\n#     inputs = tokenizer(\n#         prompt, return_tensors='pt', padding=True, max_length=512, return_token_type_ids=False\n#     )\n#     inputs = {k: v.to(model.device) for k, v in inputs.items()}\n#     res = model.generate(\n#         **inputs, max_length=512, do_sample=False, eos_token_id=tokenizer.eos_token_id\n#     )\n#     output = tokenizer.decode(res[0], skip_special_tokens=True)\n#     # Parse only what's after \"Answer:\"\n#     answer = output.split(\"Answer:\")[-1].strip()\n#     return answer\n\n# # --- 4. Example Batch Inference ---\n# input_texts = [\n#     'This LOAN AND SECURITY AGREEMENT dated January 27 , 1999 , between SILICON VALLEY BANK (\" Bank \"), a California - chartered bank with its principal place of business at 3003 Tasman Drive , Santa Clara , California 95054 with a loan production office located at 40 William St ., Ste .',\n#     \"Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano\",\n#     \"Apple Inc Chief Executive Steve Jobs sought to soothe investor concerns about his health on Monday, saying his weight loss was caused by a hormone imbalance that is relatively simple to treat.\",\n#     'april gold down 20 cents to settle at $1,116.10/oz'\n# ]\n\n# for text in input_texts:\n#     print(\"INPUT:\", text)\n#     print(\"MODEL OUTPUT:\", fingpt_ner(model, tokenizer, text))\n#     print()\n\n# --- 5. For batch evaluation on a dataset (if you have a list of sentences) ---\n# You can use a loop or tqdm for progress. Example:\n# out_text_list = [fingpt_ner(model, tokenizer, inp) for inp in tqdm(list_of_sentences)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:42:45.711854Z","iopub.execute_input":"2025-05-25T10:42:45.712099Z","iopub.status.idle":"2025-05-25T10:45:32.506585Z","shell.execute_reply.started":"2025-05-25T10:42:45.712078Z","shell.execute_reply":"2025-05-25T10:45:32.506032Z"}},"outputs":[{"name":"stderr","text":"2025-05-25 10:43:00.924285: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748169781.103418      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748169781.159421      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4f63be2d6da40e38b7a778c841e4828"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39a27cc558ac4319be8ad7b8810a9a03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c63a269a5864e6898e080747469bce0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6586e014b6f2487a83244beb0a0f3466"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5055a161fe6402ea5a3fa79aa2f6a04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea7e43e5d1bf45b1bc472a7d810a1632"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a1562c46b4f4763bc3d0db7971399e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"279d87be375a4bfe8bde62c1adbaecd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3d51b8b1a7d40b6a0a73ac8d30cdf1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a73c73acbe04208b2949f02382cec7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd453c0305954eaebe4e6672afc5e703"}},"metadata":{}},{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\nThe new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/460 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14d42b4f6c424bcbb2562444d2feba50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.bin:   0%|          | 0.00/12.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95749a9c4c1f455a855fc2d2e1eeca59"}},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"# *ZSP*","metadata":{}},{"cell_type":"code","source":"\n# !pip install seqeval\n# from datasets import load_dataset, Dataset\n\n# # 1. Load test set and add predictions\n# dataset = load_dataset(\"FinGPT/fingpt-ner\")\n# test_dataset = dataset['test']\n\n# # out_text_list = [...] # <-- This should be a list of model predictions after inference\n\n# # Example: Generate model predictions for all test samples\n# out_text_list = []\n# for sample in test_dataset:\n#     input_text = sample[\"input\"]\n#     instruction = \"Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.\"\n#     prompt = f\"Instruction: {instruction}\\nInput: {input_text}\\nAnswer: \"\n#     inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=500, return_token_type_ids=False)\n#     inputs = {k: v.to(model.device) for k, v in inputs.items()}\n#     with torch.no_grad():\n#         res = model.generate(\n#             **inputs,\n#             max_new_tokens=34,  # adjust based on your task\n#             do_sample=False,\n#             eos_token_id=tokenizer.eos_token_id\n#         )\n#     output = tokenizer.decode(res[0], skip_special_tokens=True)\n#     answer = output.split(\"Answer:\")[-1].strip()\n#     out_text_list.append(answer)\n# # Make sure test_dataset is a Hugging Face Dataset and add predictions\n# test_dataset = test_dataset.add_column(\"out_text\", out_text_list)\n\n# # 2. Your mapping and metrics code\n# import re\n# from seqeval.metrics import accuracy_score, classification_report, f1_score, precision_score, recall_score\n\n# ent_dict = {'PER': 'person', 'ORG': 'organization', 'LOC': 'location'}\n# ent_dict_rev = {v: k for k, v in ent_dict.items()}\n\n# def cvt_text_to_pred(tokens, text):\n#     preds = ['O' for _ in range(len(tokens))]\n#     for pred_txt in text.lower().strip('.').split(','):\n#         pred_match = re.match(r'^(.*) is an? (.*)$', pred_txt.strip())\n#         if pred_match is not None:\n#             entity, entity_type = pred_match.group(1).strip(), pred_match.group(2).strip()\n#             entity_pred = ent_dict_rev.get(entity_type, 'O')\n#             entity_tokens = entity.split()\n#             n = len(entity_tokens)\n#             for i in range(len(tokens) - n + 1):\n#                 if tokens[i:i+n] == entity_tokens and preds[i:i+n] == ['O'] * n:\n#                     preds[i:i+n] = ['B-' + entity_pred] + ['I-' + entity_pred] * (n-1)\n#                     break\n#     return preds\n\n# def map_output(feature):\n#     tokens = feature['input'].lower().split()\n#     label = cvt_text_to_pred(tokens, feature['output'])\n#     pred = cvt_text_to_pred(tokens, feature['out_text'])\n#     return {'label': label, 'pred': pred}\n\n# # Map and collect only matching-length samples\n# mapped = test_dataset.map(map_output)\n# labels = mapped['label']\n# preds = mapped['pred']\n\n# filtered_labels = []\n# filtered_preds = []\n# n_total = len(labels)\n# n_dropped = 0\n\n# for l, p in zip(labels, preds):\n#     if len(l) == len(p):\n#         filtered_labels.append(l)\n#         filtered_preds.append(p)\n#     else:\n#         n_dropped += 1\n\n# print(f\"Filtered out {n_dropped} of {n_total} samples due to length mismatch.\")\n\n# # Metrics\n# f1_weighted = f1_score(filtered_labels, filtered_preds, average='weighted')\n# f1_macro = f1_score(filtered_labels, filtered_preds, average='macro')\n# f1_micro = f1_score(filtered_labels, filtered_preds, average='micro')\n\n# print(f\"Weighted Avg F1: {f1_weighted:.4f}\")\n# print(f\"Macro Avg F1:    {f1_macro:.4f}\")\n# print(f\"Micro Avg F1:    {f1_micro:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:46:08.572888Z","iopub.execute_input":"2025-05-25T10:46:08.573531Z","iopub.status.idle":"2025-05-25T10:50:45.499302Z","shell.execute_reply.started":"2025-05-25T10:46:08.573506Z","shell.execute_reply":"2025-05-25T10:50:45.498674Z"}},"outputs":[{"name":"stdout","text":"Collecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.2.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2.4.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->seqeval) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=4b91e76e8937babc5eec3658a0d0f4188a155fe1fbf51d03a085453bb29100f3\n  Stored in directory: /root/.cache/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/605 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d88e16e70819411cb252458c0eea1234"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-d7135e50737d3d09.parquet:   0%|          | 0.00/76.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"453423c108584bb180da2c479b052cee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-4b7e951bae254532.parquet:   0%|          | 0.00/29.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05ee854767234f229cd4746160980cb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/511 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1704b52edb7241f48a27a35e52883712"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/98 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f314290d8e354baa85551ce2c68b389e"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/98 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d5ee5a1224046fb92f76b81bd1b15d1"}},"metadata":{}},{"name":"stdout","text":"Filtered out 0 of 98 samples due to length mismatch.\nWeighted Avg F1: 0.6976\nMacro Avg F1:    0.5367\nMicro Avg F1:    0.7024\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# *FSP*","metadata":{}},{"cell_type":"code","source":"# !pip install seqeval\n# from datasets import load_dataset, Dataset\n\n# # 1. Load test set and add predictions\n# dataset = load_dataset(\"FinGPT/fingpt-ner\")\n# test_dataset = dataset['test']\n\n# # Few-shot examples (three-shot)\n# few_shot_examples = \"\"\"Example 1:\n# Input: John works at Google in California.\n# Answer: John is a person, Google is an organization, California is a location.\n\n# Example 2:\n# Input: Microsoft hired Alice from New York.\n# Answer: Microsoft is an organization, Alice is a person, New York is a location.\n\n# Example 3:\n# Input: Emma moved to Amazon in Seattle.\n# Answer: Emma is a person, Amazon is an organization, Seattle is a location.\n# \"\"\"\n\n# # Example: Generate model predictions for all test samples\n# out_text_list = []\n# for sample in test_dataset:\n#     input_text = sample[\"input\"]\n#     instruction = \"Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.\"\n#     prompt = f\"{few_shot_examples}\\nInstruction: {instruction}\\nInput: {input_text}\\nAnswer: \"\n#     inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=500, return_token_type_ids=False)\n#     inputs = {k: v.to(model.device) for k, v in inputs.items()}\n#     with torch.no_grad():\n#         res = model.generate(\n#             **inputs,\n#             max_new_tokens=34,  # adjust based on your task\n#             do_sample=False,\n#             eos_token_id=tokenizer.eos_token_id\n#         )\n#     output = tokenizer.decode(res[0], skip_special_tokens=True)\n#     answer = output.split(\"Answer:\")[-1].strip()\n#     out_text_list.append(answer)\n# # Make sure test_dataset is a Hugging Face Dataset and add predictions\n# test_dataset = test_dataset.add_column(\"out_text\", out_text_list)\n\n# # 2. Your mapping and metrics code\n# import re\n# from seqeval.metrics import accuracy_score, classification_report, f1_score, precision_score, recall_score\n\n# ent_dict = {'PER': 'person', 'ORG': 'organization', 'LOC': 'location'}\n# ent_dict_rev = {v: k for k, v in ent_dict.items()}\n\n# def cvt_text_to_pred(tokens, text):\n#     preds = ['O' for _ in range(len(tokens))]\n#     for pred_txt in text.lower().strip('.').split(','):\n#         pred_match = re.match(r'^(.*) is an? (.*)$', pred_txt.strip())\n#         if pred_match is not None:\n#             entity, entity_type = pred_match.group(1).strip(), pred_match.group(2).strip()\n#             entity_pred = ent_dict_rev.get(entity_type, 'O')\n#             entity_tokens = entity.split()\n#             n = len(entity_tokens)\n#             for i in range(len(tokens) - n + 1):\n#                 if tokens[i:i+n] == entity_tokens and preds[i:i+n] == ['O'] * n:\n#                     preds[i:i+n] = ['B-' + entity_pred] + ['I-' + entity_pred] * (n-1)\n#                     break\n#     return preds\n\n# def map_output(feature):\n#     tokens = feature['input'].lower().split()\n#     label = cvt_text_to_pred(tokens, feature['output'])\n#     pred = cvt_text_to_pred(tokens, feature['out_text'])\n#     return {'label': label, 'pred': pred}\n\n# # Map and collect only matching-length samples\n# mapped = test_dataset.map(map_output)\n# labels = mapped['label']\n# preds = mapped['pred']\n\n# filtered_labels = []\n# filtered_preds = []\n# n_total = len(labels)\n# n_dropped = 0\n\n# for l, p in zip(labels, preds):\n#     if len(l) == len(p):\n#         filtered_labels.append(l)\n#         filtered_preds.append(p)\n#     else:\n#         n_dropped += 1\n\n# print(f\"Filtered out {n_dropped} of {n_total} samples due to length mismatch.\")\n\n# # Metrics\n# f1_weighted = f1_score(filtered_labels, filtered_preds, average='weighted')\n# f1_macro = f1_score(filtered_labels, filtered_preds, average='macro')\n# f1_micro = f1_score(filtered_labels, filtered_preds, average='micro')\n\n# print(f\"Weighted Avg F1: {f1_weighted:.4f}\")\n# print(f\"Macro Avg F1:    {f1_macro:.4f}\")\n# print(f\"Micro Avg F1:    {f1_micro:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:57:54.298359Z","iopub.execute_input":"2025-05-25T10:57:54.298739Z","iopub.status.idle":"2025-05-25T11:00:32.610928Z","shell.execute_reply.started":"2025-05-25T10:57:54.298714Z","shell.execute_reply":"2025-05-25T11:00:32.610240Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.2.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2.4.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->seqeval) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.14.0->seqeval) (2024.2.0)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/98 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3a810cc8381478ebd0ec8e37ef053ab"}},"metadata":{}},{"name":"stdout","text":"Filtered out 0 of 98 samples due to length mismatch.\nWeighted Avg F1: 0.0000\nMacro Avg F1:    0.0000\nMicro Avg F1:    0.0000\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# *COTP*","metadata":{}},{"cell_type":"code","source":"# !pip install seqeval\n# from datasets import load_dataset, Dataset\n\n# # 1. Load test set and add predictions\n# dataset = load_dataset(\"FinGPT/fingpt-ner\")\n# test_dataset = dataset['test']\n\n# # Chain of thought few-shot examples for NER\n# chain_of_thought_examples = \"\"\"Example 1:\n# Input: John works at Google in California.\n# Let's think step by step:\n# - Identify entities: John, Google, California.\n# - Determine type for each entity:\n#   - John is a person's name.\n#   - Google is a company/organization.\n#   - California is a place/location.\n# Answer: John is a person, Google is an organization, California is a location.\n\n# Example 2:\n# Input: Microsoft hired Alice from New York.\n# Let's think step by step:\n# - Identify entities: Microsoft, Alice, New York.\n# - Determine type for each entity:\n#   - Microsoft is an organization.\n#   - Alice is a person's name.\n#   - New York is a location.\n# Answer: Microsoft is an organization, Alice is a person, New York is a location.\n\n# Example 3:\n# Input: Emma moved to Amazon in Seattle.\n# Let's think step by step:\n# - Identify entities: Emma, Amazon, Seattle.\n# - Determine type for each entity:\n#   - Emma is a person's name.\n#   - Amazon is an organization.\n#   - Seattle is a location.\n# Answer: Emma is a person, Amazon is an organization, Seattle is a location.\n# \"\"\"\n\n# # Generate model predictions for all test samples using chain of thought prompting\n# out_text_list = []\n# for sample in test_dataset:\n#     input_text = sample[\"input\"]\n#     instruction = \"Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.\"\n#     prompt = f\"\"\"{chain_of_thought_examples}\\nInstruction: {instruction}\\nInput: {input_text}\\nLet's think step by step:\"\"\"\n#     inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=500, return_token_type_ids=False)\n#     inputs = {k: v.to(model.device) for k, v in inputs.items()}\n#     with torch.no_grad():\n#         res = model.generate(\n#             **inputs,\n#             max_new_tokens=60,  # allow for chain-of-thought output\n#             do_sample=False,\n#             eos_token_id=tokenizer.eos_token_id\n#         )\n#     output = tokenizer.decode(res[0], skip_special_tokens=True)\n#     # Extract answer after last \"Answer:\" in generated output\n#     answer = output.split(\"Answer:\")[-1].strip() if \"Answer:\" in output else output\n#     out_text_list.append(answer)\n# # Make sure test_dataset is a Hugging Face Dataset and add predictions\n# test_dataset = test_dataset.add_column(\"out_text\", out_text_list)\n\n# # 2. Your mapping and metrics code\n# import re\n# from seqeval.metrics import accuracy_score, classification_report, f1_score, precision_score, recall_score\n\n# ent_dict = {'PER': 'person', 'ORG': 'organization', 'LOC': 'location'}\n# ent_dict_rev = {v: k for k, v in ent_dict.items()}\n\n# def cvt_text_to_pred(tokens, text):\n#     preds = ['O' for _ in range(len(tokens))]\n#     for pred_txt in text.lower().strip('.').split(','):\n#         pred_match = re.match(r'^(.*) is an? (.*)$', pred_txt.strip())\n#         if pred_match is not None:\n#             entity, entity_type = pred_match.group(1).strip(), pred_match.group(2).strip()\n#             entity_pred = ent_dict_rev.get(entity_type, 'O')\n#             entity_tokens = entity.split()\n#             n = len(entity_tokens)\n#             for i in range(len(tokens) - n + 1):\n#                 if tokens[i:i+n] == entity_tokens and preds[i:i+n] == ['O'] * n:\n#                     preds[i:i+n] = ['B-' + entity_pred] + ['I-' + entity_pred] * (n-1)\n#                     break\n#     return preds\n\n# def map_output(feature):\n#     tokens = feature['input'].lower().split()\n#     label = cvt_text_to_pred(tokens, feature['output'])\n#     pred = cvt_text_to_pred(tokens, feature['out_text'])\n#     return {'label': label, 'pred': pred}\n\n# # Map and collect only matching-length samples\n# mapped = test_dataset.map(map_output)\n# labels = mapped['label']\n# preds = mapped['pred']\n\n# filtered_labels = []\n# filtered_preds = []\n# n_total = len(labels)\n# n_dropped = 0\n\n# for l, p in zip(labels, preds):\n#     if len(l) == len(p):\n#         filtered_labels.append(l)\n#         filtered_preds.append(p)\n#     else:\n#         n_dropped += 1\n\n# print(f\"Filtered out {n_dropped} of {n_total} samples due to length mismatch.\")\n\n# # Metrics\n# f1_weighted = f1_score(filtered_labels, filtered_preds, average='weighted')\n# f1_macro = f1_score(filtered_labels, filtered_preds, average='macro')\n# f1_micro = f1_score(filtered_labels, filtered_preds, average='micro')\n\n# print(f\"Weighted Avg F1: {f1_weighted:.4f}\")\n# print(f\"Macro Avg F1:    {f1_macro:.4f}\")\n# print(f\"Micro Avg F1:    {f1_micro:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:01:44.931184Z","iopub.execute_input":"2025-05-25T11:01:44.931873Z","iopub.status.idle":"2025-05-25T11:06:38.659197Z","shell.execute_reply.started":"2025-05-25T11:01:44.931843Z","shell.execute_reply":"2025-05-25T11:06:38.658358Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.2.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2.4.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->seqeval) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.14.0->seqeval) (2024.2.0)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/98 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea4952b49ef34a748add7a2cc37738eb"}},"metadata":{}},{"name":"stdout","text":"Filtered out 0 of 98 samples due to length mismatch.\nWeighted Avg F1: 0.0000\nMacro Avg F1:    0.0000\nMicro Avg F1:    0.0000\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# *MP*","metadata":{}},{"cell_type":"code","source":"# !pip install seqeval\n# from datasets import load_dataset, Dataset\n\n# # 1. Load test set and add predictions\n# dataset = load_dataset(\"FinGPT/fingpt-ner\")\n# test_dataset = dataset['test']\n\n# # Meta prompting few-shot examples for NER\n# meta_prompting_examples = \"\"\"Case 1:\n# Sentence: John works at Google in California.\n# Meta Analysis: \n# - Does the sentence contain names of people? → Yes (\"John\")\n# - Does it mention organizations? → Yes (\"Google\")\n# - Does it refer to locations? → Yes (\"California\")\n# Entities and types: John is a person, Google is an organization, California is a location.\n\n# Case 2:\n# Sentence: Microsoft hired Alice from New York.\n# Meta Analysis: \n# - Does the sentence contain names of people? → Yes (\"Alice\")\n# - Does it mention organizations? → Yes (\"Microsoft\")\n# - Does it refer to locations? → Yes (\"New York\")\n# Entities and types: Microsoft is an organization, Alice is a person, New York is a location.\n\n# Case 3:\n# Sentence: Emma moved to Amazon in Seattle.\n# Meta Analysis:\n# - Does the sentence contain names of people? → Yes (\"Emma\")\n# - Does it mention organizations? → Yes (\"Amazon\")\n# - Does it refer to locations? → Yes (\"Seattle\")\n# Entities and types: Emma is a person, Amazon is an organization, Seattle is a location.\n# \"\"\"\n\n# # Generate model predictions for all test samples using meta prompting\n# out_text_list = []\n# for sample in test_dataset:\n#     input_text = sample[\"input\"]\n#     instruction = \"Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.\"\n#     prompt = f\"\"\"{meta_prompting_examples}\\nInstruction: {instruction}\\nSentence: {input_text}\\nMeta Analysis:\"\"\"\n#     inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=500, return_token_type_ids=False)\n#     inputs = {k: v.to(model.device) for k, v in inputs.items()}\n#     with torch.no_grad():\n#         res = model.generate(\n#             **inputs,\n#             max_new_tokens=60,  # allow for meta-analytical output\n#             do_sample=False,\n#             eos_token_id=tokenizer.eos_token_id\n#         )\n#     output = tokenizer.decode(res[0], skip_special_tokens=True)\n#     # Extract entities after last \"Entities and types:\" in generated output\n#     answer = output.split(\"Entities and types:\")[-1].strip() if \"Entities and types:\" in output else output\n#     out_text_list.append(answer)\n# # Make sure test_dataset is a Hugging Face Dataset and add predictions\n# test_dataset = test_dataset.add_column(\"out_text\", out_text_list)\n\n# # 2. Your mapping and metrics code\n# import re\n# from seqeval.metrics import accuracy_score, classification_report, f1_score, precision_score, recall_score\n\n# ent_dict = {'PER': 'person', 'ORG': 'organization', 'LOC': 'location'}\n# ent_dict_rev = {v: k for k, v in ent_dict.items()}\n\n# def cvt_text_to_pred(tokens, text):\n#     preds = ['O' for _ in range(len(tokens))]\n#     for pred_txt in text.lower().strip('.').split(','):\n#         pred_match = re.match(r'^(.*) is an? (.*)$', pred_txt.strip())\n#         if pred_match is not None:\n#             entity, entity_type = pred_match.group(1).strip(), pred_match.group(2).strip()\n#             entity_pred = ent_dict_rev.get(entity_type, 'O')\n#             entity_tokens = entity.split()\n#             n = len(entity_tokens)\n#             for i in range(len(tokens) - n + 1):\n#                 if tokens[i:i+n] == entity_tokens and preds[i:i+n] == ['O'] * n:\n#                     preds[i:i+n] = ['B-' + entity_pred] + ['I-' + entity_pred] * (n-1)\n#                     break\n#     return preds\n\n# def map_output(feature):\n#     tokens = feature['input'].lower().split()\n#     label = cvt_text_to_pred(tokens, feature['output'])\n#     pred = cvt_text_to_pred(tokens, feature['out_text'])\n#     return {'label': label, 'pred': pred}\n\n# # Map and collect only matching-length samples\n# mapped = test_dataset.map(map_output)\n# labels = mapped['label']\n# preds = mapped['pred']\n\n# filtered_labels = []\n# filtered_preds = []\n# n_total = len(labels)\n# n_dropped = 0\n\n# for l, p in zip(labels, preds):\n#     if len(l) == len(p):\n#         filtered_labels.append(l)\n#         filtered_preds.append(p)\n#     else:\n#         n_dropped += 1\n\n# print(f\"Filtered out {n_dropped} of {n_total} samples due to length mismatch.\")\n\n# # Metrics\n# f1_weighted = f1_score(filtered_labels, filtered_preds, average='weighted')\n# f1_macro = f1_score(filtered_labels, filtered_preds, average='macro')\n# f1_micro = f1_score(filtered_labels, filtered_preds, average='micro')\n\n# print(f\"Weighted Avg F1: {f1_weighted:.4f}\")\n# print(f\"Macro Avg F1:    {f1_macro:.4f}\")\n# print(f\"Micro Avg F1:    {f1_micro:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:08:27.361547Z","iopub.execute_input":"2025-05-25T11:08:27.362264Z","iopub.status.idle":"2025-05-25T11:12:19.629438Z","shell.execute_reply.started":"2025-05-25T11:08:27.362239Z","shell.execute_reply":"2025-05-25T11:12:19.628776Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.2.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2.4.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->seqeval) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.14.0->seqeval) (2024.2.0)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/98 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e3b52f70d2b4a53b1d47c2a12f8546c"}},"metadata":{}},{"name":"stdout","text":"Filtered out 0 of 98 samples due to length mismatch.\nWeighted Avg F1: 0.0000\nMacro Avg F1:    0.0000\nMicro Avg F1:    0.0000\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# *GKP*","metadata":{}},{"cell_type":"code","source":"# !pip install seqeval\n# from datasets import load_dataset, Dataset\n\n# # 1. Load test set and add predictions\n# dataset = load_dataset(\"FinGPT/fingpt-ner\")\n# test_dataset = dataset['test']\n\n# # Generated knowledge prompting few-shot examples for NER\n# knowledge_prompting_examples = \"\"\"Case 1:\n# Sentence: John works at Google in California.\n# Knowledge Reasoning:\n# - 'John' is a proper noun and commonly used as a person's name.\n# - 'Google' is widely recognized as a company/organization.\n# - 'California' is a well-known location/geographical area.\n# Extracted Entities: John is a person, Google is an organization, California is a location.\n\n# Case 2:\n# Sentence: Microsoft hired Alice from New York.\n# Knowledge Reasoning:\n# - 'Microsoft' is a famous technology company.\n# - 'Alice' is a typical personal name.\n# - 'New York' refers to a major city/state, a location.\n# Extracted Entities: Microsoft is an organization, Alice is a person, New York is a location.\n\n# Case 3:\n# Sentence: Emma moved to Amazon in Seattle.\n# Knowledge Reasoning:\n# - 'Emma' is a common person's name.\n# - 'Amazon' is a well-known organization.\n# - 'Seattle' is a recognized city, a location.\n# Extracted Entities: Emma is a person, Amazon is an organization, Seattle is a location.\n# \"\"\"\n\n# # Generate model predictions for all test samples using generated knowledge prompting\n# out_text_list = []\n# for sample in test_dataset:\n#     input_text = sample[\"input\"]\n#     instruction = \"Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.\"\n#     prompt = f\"\"\"{knowledge_prompting_examples}\\nInstruction: {instruction}\\nSentence: {input_text}\\nKnowledge Reasoning:\"\"\"\n#     inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=500, return_token_type_ids=False)\n#     inputs = {k: v.to(model.device) for k, v in inputs.items()}\n#     with torch.no_grad():\n#         res = model.generate(\n#             **inputs,\n#             max_new_tokens=60,  # allow for detailed reasoning\n#             do_sample=False,\n#             eos_token_id=tokenizer.eos_token_id\n#         )\n#     output = tokenizer.decode(res[0], skip_special_tokens=True)\n#     # Extract answer after last \"Extracted Entities:\" in generated output\n#     answer = output.split(\"Extracted Entities:\")[-1].strip() if \"Extracted Entities:\" in output else output\n#     out_text_list.append(answer)\n# # Make sure test_dataset is a Hugging Face Dataset and add predictions\n# test_dataset = test_dataset.add_column(\"out_text\", out_text_list)\n\n# # 2. Your mapping and metrics code\n# import re\n# from seqeval.metrics import accuracy_score, classification_report, f1_score, precision_score, recall_score\n\n# ent_dict = {'PER': 'person', 'ORG': 'organization', 'LOC': 'location'}\n# ent_dict_rev = {v: k for k, v in ent_dict.items()}\n\n# def cvt_text_to_pred(tokens, text):\n#     preds = ['O' for _ in range(len(tokens))]\n#     for pred_txt in text.lower().strip('.').split(','):\n#         pred_match = re.match(r'^(.*) is an? (.*)$', pred_txt.strip())\n#         if pred_match is not None:\n#             entity, entity_type = pred_match.group(1).strip(), pred_match.group(2).strip()\n#             entity_pred = ent_dict_rev.get(entity_type, 'O')\n#             entity_tokens = entity.split()\n#             n = len(entity_tokens)\n#             for i in range(len(tokens) - n + 1):\n#                 if tokens[i:i+n] == entity_tokens and preds[i:i+n] == ['O'] * n:\n#                     preds[i:i+n] = ['B-' + entity_pred] + ['I-' + entity_pred] * (n-1)\n#                     break\n#     return preds\n\n# def map_output(feature):\n#     tokens = feature['input'].lower().split()\n#     label = cvt_text_to_pred(tokens, feature['output'])\n#     pred = cvt_text_to_pred(tokens, feature['out_text'])\n#     return {'label': label, 'pred': pred}\n\n# # Map and collect only matching-length samples\n# mapped = test_dataset.map(map_output)\n# labels = mapped['label']\n# preds = mapped['pred']\n\n# filtered_labels = []\n# filtered_preds = []\n# n_total = len(labels)\n# n_dropped = 0\n\n# for l, p in zip(labels, preds):\n#     if len(l) == len(p):\n#         filtered_labels.append(l)\n#         filtered_preds.append(p)\n#     else:\n#         n_dropped += 1\n\n# print(f\"Filtered out {n_dropped} of {n_total} samples due to length mismatch.\")\n\n# # Metrics\n# f1_weighted = f1_score(filtered_labels, filtered_preds, average='weighted')\n# f1_macro = f1_score(filtered_labels, filtered_preds, average='macro')\n# f1_micro = f1_score(filtered_labels, filtered_preds, average='micro')\n\n# print(f\"Weighted Avg F1: {f1_weighted:.4f}\")\n# print(f\"Macro Avg F1:    {f1_macro:.4f}\")\n# print(f\"Micro Avg F1:    {f1_micro:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:16:34.698744Z","iopub.execute_input":"2025-05-25T12:16:34.699065Z","iopub.status.idle":"2025-05-25T12:21:08.554887Z","shell.execute_reply.started":"2025-05-25T12:16:34.699036Z","shell.execute_reply":"2025-05-25T12:21:08.554009Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.2.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2.4.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->seqeval) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.14.0->seqeval) (2024.2.0)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/98 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0940f816d1f7477da01dfacf46c172bc"}},"metadata":{}},{"name":"stdout","text":"Filtered out 0 of 98 samples due to length mismatch.\nWeighted Avg F1: 0.0000\nMacro Avg F1:    0.0000\nMicro Avg F1:    0.0000\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# *PC*","metadata":{}},{"cell_type":"code","source":"!pip install seqeval\nfrom datasets import load_dataset, Dataset\n\n# 1. Load test set and add predictions\ndataset = load_dataset(\"FinGPT/fingpt-ner\")\ntest_dataset = dataset['test']\n\n# Prompt chaining few-shot examples for NER\nprompt_chaining_examples = \"\"\"Example 1:\nStep 1 Output: The sentence is \"John works at Google in California.\"\nStep 2 Input: The sentence is \"John works at Google in California.\"\nStep 2 Output: Identified entities: John, Google, California.\nStep 3 Input: Identified entities: John, Google, California.\nStep 3 Output: Type assignment:\n- John is a person.\n- Google is an organization.\n- California is a location.\nFinal Answer: John is a person, Google is an organization, California is a location.\n\nExample 2:\nStep 1 Output: The sentence is \"Microsoft hired Alice from New York.\"\nStep 2 Input: The sentence is \"Microsoft hired Alice from New York.\"\nStep 2 Output: Identified entities: Microsoft, Alice, New York.\nStep 3 Input: Identified entities: Microsoft, Alice, New York.\nStep 3 Output: Type assignment:\n- Microsoft is an organization.\n- Alice is a person.\n- New York is a location.\nFinal Answer: Microsoft is an organization, Alice is a person, New York is a location.\n\nExample 3:\nStep 1 Output: The sentence is \"Emma moved to Amazon in Seattle.\"\nStep 2 Input: The sentence is \"Emma moved to Amazon in Seattle.\"\nStep 2 Output: Identified entities: Emma, Amazon, Seattle.\nStep 3 Input: Identified entities: Emma, Amazon, Seattle.\nStep 3 Output: Type assignment:\n- Emma is a person.\n- Amazon is an organization.\n- Seattle is a location.\nFinal Answer: Emma is a person, Amazon is an organization, Seattle is a location.\n\"\"\"\n\n# Generate model predictions for all test samples using prompt chaining\nout_text_list = []\nfor sample in test_dataset:\n    input_text = sample[\"input\"]\n    instruction = \"Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.\"\n    prompt = f\"\"\"{prompt_chaining_examples}\\nInstruction: {instruction}\\nStep 1 Output: The sentence is \"{input_text}\"\\nStep 2 Input: The sentence is \"{input_text}\"\\nStep 2 Output:\"\"\"\n    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=500, return_token_type_ids=False)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    with torch.no_grad():\n        res = model.generate(\n            **inputs,\n            max_new_tokens=80,  # allow for chaining steps output\n            do_sample=False,\n            eos_token_id=tokenizer.eos_token_id\n        )\n    output = tokenizer.decode(res[0], skip_special_tokens=True)\n    # Extract answer after last \"Final Answer:\" in generated output\n    answer = output.split(\"Final Answer:\")[-1].strip() if \"Final Answer:\" in output else output\n    out_text_list.append(answer)\n# Make sure test_dataset is a Hugging Face Dataset and add predictions\ntest_dataset = test_dataset.add_column(\"out_text\", out_text_list)\n\n# 2. Your mapping and metrics code\nimport re\nfrom seqeval.metrics import accuracy_score, classification_report, f1_score, precision_score, recall_score\n\nent_dict = {'PER': 'person', 'ORG': 'organization', 'LOC': 'location'}\nent_dict_rev = {v: k for k, v in ent_dict.items()}\n\ndef cvt_text_to_pred(tokens, text):\n    preds = ['O' for _ in range(len(tokens))]\n    for pred_txt in text.lower().strip('.').split(','):\n        pred_match = re.match(r'^(.*) is an? (.*)$', pred_txt.strip())\n        if pred_match is not None:\n            entity, entity_type = pred_match.group(1).strip(), pred_match.group(2).strip()\n            entity_pred = ent_dict_rev.get(entity_type, 'O')\n            entity_tokens = entity.split()\n            n = len(entity_tokens)\n            for i in range(len(tokens) - n + 1):\n                if tokens[i:i+n] == entity_tokens and preds[i:i+n] == ['O'] * n:\n                    preds[i:i+n] = ['B-' + entity_pred] + ['I-' + entity_pred] * (n-1)\n                    break\n    return preds\n\ndef map_output(feature):\n    tokens = feature['input'].lower().split()\n    label = cvt_text_to_pred(tokens, feature['output'])\n    pred = cvt_text_to_pred(tokens, feature['out_text'])\n    return {'label': label, 'pred': pred}\n\n# Map and collect only matching-length samples\nmapped = test_dataset.map(map_output)\nlabels = mapped['label']\npreds = mapped['pred']\n\nfiltered_labels = []\nfiltered_preds = []\nn_total = len(labels)\nn_dropped = 0\n\nfor l, p in zip(labels, preds):\n    if len(l) == len(p):\n        filtered_labels.append(l)\n        filtered_preds.append(p)\n    else:\n        n_dropped += 1\n\nprint(f\"Filtered out {n_dropped} of {n_total} samples due to length mismatch.\")\n\n# Metrics\nf1_weighted = f1_score(filtered_labels, filtered_preds, average='weighted')\nf1_macro = f1_score(filtered_labels, filtered_preds, average='macro')\nf1_micro = f1_score(filtered_labels, filtered_preds, average='micro')\n\nprint(f\"Weighted Avg F1: {f1_weighted:.4f}\")\nprint(f\"Macro Avg F1:    {f1_macro:.4f}\")\nprint(f\"Micro Avg F1:    {f1_micro:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# # Your previously computed metrics\n# metrics = ['Weighted F1', 'Macro F1', 'Micro F1']\n# values = [f1_weighted, f1_macro, f1_micro]\n\n# plt.figure(figsize=(7, 5))\n# bars = plt.bar(metrics, values, color=['#4c72b0', '#55a868', '#c44e52'])\n# plt.ylim([0, 1])\n# plt.title(\"NER Model Evaluation Metrics\")\n# plt.ylabel(\"Score\")\n# for bar, v in zip(bars, values):\n#     plt.text(bar.get_x() + bar.get_width() / 2, v + 0.02, f\"{v:.2f}\", ha='center', va='bottom', fontsize=12)\n# plt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-25T10:42:14.102Z"}},"outputs":[],"execution_count":null}]}