{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # Cell 1: Install dependencies (for Colab/Notebook)\n# !pip install datasets transformers accelerate torch scikit-learn tqdm peft --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:53:43.958990Z","iopub.execute_input":"2025-05-25T14:53:43.959188Z","iopub.status.idle":"2025-05-25T14:53:43.963506Z","shell.execute_reply.started":"2025-05-25T14:53:43.959171Z","shell.execute_reply":"2025-05-25T14:53:43.962757Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# # Cell 2: Imports and login\n# import warnings\n# warnings.filterwarnings(\"ignore\")\n\n# import torch\n# from transformers import AutoTokenizer, AutoModelForCausalLM\n# from datasets import load_dataset\n# from tqdm import tqdm\n# import pandas as pd\n\n# # Hugging Face login (use environment variable for safety)\n# from huggingface_hub import login\n# import os\n# HF_TOKEN = os.getenv(\"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\", \"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\")  # Replace with your token or set as env var\n# login(HF_TOKEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:53:43.964233Z","iopub.execute_input":"2025-05-25T14:53:43.964479Z","iopub.status.idle":"2025-05-25T14:53:43.988485Z","shell.execute_reply.started":"2025-05-25T14:53:43.964457Z","shell.execute_reply":"2025-05-25T14:53:43.987825Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# # Cell 3: Load model and adapter\n\n# base_model = \"meta-llama/Llama-2-7b-hf\"\n# peft_model = \"FinGPT/fingpt-mt_llama2-7b_lora\"\n\n# from peft import PeftModel\n\n# tokenizer = AutoTokenizer.from_pretrained(base_model)\n# model = AutoModelForCausalLM.from_pretrained(\n#     base_model,\n#     torch_dtype=torch.float16,\n#     device_map=\"auto\"\n# )\n# model = PeftModel.from_pretrained(model, peft_model)\n# model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:53:43.989185Z","iopub.execute_input":"2025-05-25T14:53:43.989348Z","iopub.status.idle":"2025-05-25T14:53:44.003905Z","shell.execute_reply.started":"2025-05-25T14:53:43.989335Z","shell.execute_reply":"2025-05-25T14:53:44.003276Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# *ZSP*","metadata":{}},{"cell_type":"code","source":"# # Cell 4: Load and prep dataset\n\n# dataset = load_dataset(\"FinGPT/fingpt-convfinqa\", split=\"test[:200]\")\n\n\n# def build_prompt(example):\n#     return (\n#         \"You are a financial expert. Only provide the numeric answer, with no explanation.\\n\"\n#         f\"Question: {example.get('input', '')}\\n\"\n#         \"Answer:\"\n#     )\n\n\n# prompts = [build_prompt(ex) for ex in dataset]\n# targets = [ex[\"output\"] for ex in dataset]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:53:44.005724Z","iopub.execute_input":"2025-05-25T14:53:44.005946Z","iopub.status.idle":"2025-05-25T14:53:44.026997Z","shell.execute_reply.started":"2025-05-25T14:53:44.005930Z","shell.execute_reply":"2025-05-25T14:53:44.026284Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# *FSP*","metadata":{}},{"cell_type":"code","source":"# from datasets import load_dataset\n\n# dataset = load_dataset(\"FinGPT/fingpt-convfinqa\", split=\"test[:200]\")\n\n# # Select 3 examples for few-shot context (you can randomize or pick the first 3)\n# few_shot_examples = [dataset[i] for i in range(3)]\n\n# def build_few_shot_prompt(example, shots):\n#     # Build the few-shot context string\n#     context = \"\"\n#     for shot in shots:\n#         context += (\n#             \"You are a financial expert. Only provide the numeric answer, with no explanation.\\n\"\n#             f\"Question: {shot.get('input', '')}\\n\"\n#             f\"Answer: {shot.get('output', '')}\\n\\n\"\n#         )\n#     # Now add the new example (without its output)\n#     context += (\n#         \"You are a financial expert. Only provide the numeric answer, with no explanation.\\n\"\n#         f\"Question: {example.get('input', '')}\\n\"\n#         \"Answer:\"\n#     )\n#     return context\n\n# prompts = [build_few_shot_prompt(ex, few_shot_examples) for ex in dataset]\n# targets = [ex[\"output\"] for ex in dataset]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:53:44.027769Z","iopub.execute_input":"2025-05-25T14:53:44.028023Z","iopub.status.idle":"2025-05-25T14:53:44.061237Z","shell.execute_reply.started":"2025-05-25T14:53:44.028000Z","shell.execute_reply":"2025-05-25T14:53:44.060445Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# *COTP*","metadata":{}},{"cell_type":"code","source":"# from datasets import load_dataset\n\n# dataset = load_dataset(\"FinGPT/fingpt-convfinqa\", split=\"test[:200]\")\n\n# # You may want to select a few examples for demonstration, but chain-of-thought prompts usually include step-by-step reasoning.\n# def build_chain_of_thought_prompt(example):\n#     return (\n#         \"You are a financial expert. Think step by step to solve the problem. After your reasoning, provide ONLY the numeric answer on a new line, with no explanation.\\n\"\n#         f\"Question: {example.get('input', '')}\\n\"\n#         \"Let's think step by step.\\n\"\n#         \"Answer:\"\n#     )\n\n# prompts = [build_chain_of_thought_prompt(ex) for ex in dataset]\n# targets = [ex[\"output\"] for ex in dataset]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:53:44.062030Z","iopub.execute_input":"2025-05-25T14:53:44.062253Z","iopub.status.idle":"2025-05-25T14:53:44.090563Z","shell.execute_reply.started":"2025-05-25T14:53:44.062233Z","shell.execute_reply":"2025-05-25T14:53:44.089872Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# *MP*","metadata":{}},{"cell_type":"code","source":"# from datasets import load_dataset\n\n# dataset = load_dataset(\"FinGPT/fingpt-convfinqa\", split=\"test[:200]\")\n\n# def build_meta_prompt(example):\n#     return (\n#         \"Instruction: You are an advanced AI model designed to answer financial questions. \"\n#         \"You must strictly follow these rules:\\n\"\n#         \"1. Provide only the numeric answer required by the question.\\n\"\n#         \"2. Do not include any explanations, units, or extra text.\\n\"\n#         \"3. If intermediate steps are needed, perform them silently and output only the final answer.\\n\"\n#         f\"Question: {example.get('input', '')}\\n\"\n#         \"Answer:\"\n#     )\n\n# prompts = [build_meta_prompt(ex) for ex in dataset]\n# targets = [ex[\"output\"] for ex in dataset]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:53:44.091337Z","iopub.execute_input":"2025-05-25T14:53:44.091547Z","iopub.status.idle":"2025-05-25T14:53:44.120231Z","shell.execute_reply.started":"2025-05-25T14:53:44.091528Z","shell.execute_reply":"2025-05-25T14:53:44.119539Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# *GKP*","metadata":{}},{"cell_type":"code","source":"# from datasets import load_dataset\n\n# dataset = load_dataset(\"FinGPT/fingpt-convfinqa\", split=\"test[:200]\")\n\n# def build_generated_knowledge_prompt(example):\n#     return (\n#         \"You are a financial expert. Use your background knowledge about finance to reason through the question. \"\n#         \"If relevant, recall facts, formulas, or concepts from finance to support your answer. \"\n#         \"After reasoning, provide only the numeric answer on a new line, with no explanation.\\n\"\n#         f\"Question: {example.get('input', '')}\\n\"\n#         \"Let's recall relevant financial knowledge and solve the problem step by step.\\n\"\n#         \"Answer:\"\n#     )\n\n# prompts = [build_generated_knowledge_prompt(ex) for ex in dataset]\n# targets = [ex[\"output\"] for ex in dataset]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:53:44.120992Z","iopub.execute_input":"2025-05-25T14:53:44.121178Z","iopub.status.idle":"2025-05-25T14:53:44.137405Z","shell.execute_reply.started":"2025-05-25T14:53:44.121154Z","shell.execute_reply":"2025-05-25T14:53:44.136626Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# *PC*","metadata":{}},{"cell_type":"code","source":"# from datasets import load_dataset\n\n# dataset = load_dataset(\"FinGPT/fingpt-convfinqa\", split=\"test[:200]\")\n\n# def build_prompt_chaining(example):\n#     # Step 1: Extract relevant numbers or entities from the question\n#     step1 = (\n#         f\"Step 1: Identify the main numbers, entities, or facts in the question.\\n\"\n#         f\"Question: {example.get('input', '')}\\n\"\n#         \"Extracted facts:\"\n#     )\n#     # Step 2: Instruct to perform the necessary calculations or reasoning\n#     step2 = (\n#         \"Step 2: Using the extracted facts, perform the necessary calculations or reasoning to solve the problem.\\n\"\n#         \"Show your work:\"\n#     )\n#     # Step 3: Ask for the final answer only\n#     step3 = (\n#         \"Step 3: Based on your reasoning, provide ONLY the final numeric answer with no explanation.\\n\"\n#         \"Answer:\"\n#     )\n#     return f\"{step1}\\n\\n{step2}\\n\\n{step3}\"\n\n# prompts = [build_prompt_chaining(ex) for ex in dataset]\n# targets = [ex[\"output\"] for ex in dataset]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:53:44.138608Z","iopub.execute_input":"2025-05-25T14:53:44.138849Z","iopub.status.idle":"2025-05-25T14:53:44.163128Z","shell.execute_reply.started":"2025-05-25T14:53:44.138827Z","shell.execute_reply":"2025-05-25T14:53:44.162290Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# import re\n\n# def extract_closest_number(answer, ground_truth):\n#     numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", answer)\n#     if not numbers:\n#         return answer.strip()\n#     try:\n#         gt = float(ground_truth.replace(',', ''))\n#         best_num = min(numbers, key=lambda x: abs(float(x) - gt))\n#         return best_num\n#     except Exception:\n#         return numbers[0]\n\n# # Get the first 10 samples as a dataset\n# dataset_10 = dataset.select(range(200))\n\n# # Build prompts and targets: first line, dotted line, then instruction\n# prompts_10 = []\n# for ex in dataset_10:\n#     first_line = ex['input'].strip().split('\\n', 1)[0]\n#     prompts_10.append(f\"{first_line}\\n...\\nAnswer with a single number only:\")\n\n# targets_10 = [ex[\"output\"] for ex in dataset_10]\n\n# if tokenizer.pad_token is None:\n#     tokenizer.pad_token = tokenizer.eos_token\n\n# batch_size = 8\n# out_texts_10 = []\n# total_steps = len(prompts_10) // batch_size + int(len(prompts_10) % batch_size != 0)\n\n# with torch.no_grad():\n#     for i in range(total_steps):\n#         batch_prompts = prompts_10[i * batch_size:(i + 1) * batch_size]\n#         if not batch_prompts:\n#             continue\n#         tokens = tokenizer(\n#             batch_prompts,\n#             return_tensors='pt',\n#             padding=True,\n#             max_length=1012,\n#             truncation=True\n#         )\n#         tokens = {k: v.to(model.device) for k, v in tokens.items()}\n#         outputs = model.generate(\n#             **tokens,\n#             max_new_tokens=34,\n#             eos_token_id=tokenizer.eos_token_id,\n#             temperature=0.7,\n#             num_beams=2\n#         )\n#         decoded_outputs = [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n#         for prompt, decoded, target in zip(batch_prompts, decoded_outputs, targets_10[i * batch_size:(i + 1) * batch_size]):\n#             answer = decoded[len(prompt):].strip() if decoded.startswith(prompt) else decoded.strip()\n#             extracted = extract_closest_number(answer, target)\n#             out_texts_10.append(extracted)\n#            # print(f\"\\nPrompt: {prompt}\")\n#             print(f\"Ground Truth: {target}\")\n#             #print(f\"Model Output: {decoded}\")\n#             print(f\"Extracted Answer: {extract_closest_number(decoded, target)}\")\n#             print(\"-\" * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:53:44.164027Z","iopub.execute_input":"2025-05-25T14:53:44.164568Z","iopub.status.idle":"2025-05-25T14:53:44.182460Z","shell.execute_reply.started":"2025-05-25T14:53:44.164544Z","shell.execute_reply":"2025-05-25T14:53:44.181745Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# def is_number(x):\n#     try:\n#         float(x.replace(',', ''))\n#         return True\n#     except Exception:\n#         return False\n\n# y_true, y_pred = [], []\n# for t, p in zip(targets_10, out_texts_10):  # Use your actual variables\n#     if is_number(t) and is_number(p):\n#         y_true.append(float(t.replace(',', '')))\n#         y_pred.append(float(p.replace(',', '')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:53:44.183150Z","iopub.execute_input":"2025-05-25T14:53:44.183307Z","iopub.status.idle":"2025-05-25T14:53:44.210495Z","shell.execute_reply.started":"2025-05-25T14:53:44.183294Z","shell.execute_reply":"2025-05-25T14:53:44.209781Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# from sklearn.metrics import f1_score\n\n# # Convert to string for exact match F1\n# y_true_str = [str(x) for x in y_true]\n# y_pred_str = [str(x) for x in y_pred]\n\n# f1_weighted = f1_score(y_true_str, y_pred_str, average='weighted', zero_division=0)\n# f1_macro = f1_score(y_true_str, y_pred_str, average='macro', zero_division=0)\n# f1_micro = f1_score(y_true_str, y_pred_str, average='micro', zero_division=0)\n\n# print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n# print(f\"F1 Score (macro): {f1_macro:.4f}\")\n# print(f\"F1 Score (micro): {f1_micro:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:53:44.211282Z","iopub.execute_input":"2025-05-25T14:53:44.211497Z","iopub.status.idle":"2025-05-25T14:53:44.229510Z","shell.execute_reply.started":"2025-05-25T14:53:44.211480Z","shell.execute_reply":"2025-05-25T14:53:44.228882Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# *Evaluation on the second dataset: ChanceFocus/flare-finqa*","metadata":{}},{"cell_type":"code","source":"# Cell 1: Install dependencies (for Colab/Notebook)\n!pip install datasets transformers accelerate torch scikit-learn tqdm peft --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T18:58:56.782444Z","iopub.execute_input":"2025-05-25T18:58:56.783156Z","iopub.status.idle":"2025-05-25T19:00:27.693409Z","shell.execute_reply.started":"2025-05-25T18:58:56.783120Z","shell.execute_reply":"2025-05-25T19:00:27.692689Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 2: Imports and Hugging Face login\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport pandas as pd\n\n# Hugging Face login (use environment variable for safety)\nfrom huggingface_hub import login\nimport os\nlogin(\"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\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:00:27.694814Z","iopub.execute_input":"2025-05-25T19:00:27.695031Z","iopub.status.idle":"2025-05-25T19:00:44.929774Z","shell.execute_reply.started":"2025-05-25T19:00:27.695003Z","shell.execute_reply":"2025-05-25T19:00:44.929035Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Cell 3: Load model and adapter\nbase_model = \"meta-llama/Llama-2-7b-hf\"\npeft_model = \"FinGPT/fingpt-mt_llama2-7b_lora\"\n\nfrom peft import PeftModel\n\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\nmodel = PeftModel.from_pretrained(model, peft_model)\nprint(\"PEFT config:\", getattr(model, \"peft_config\", None))\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:00:44.930613Z","iopub.execute_input":"2025-05-25T19:00:44.931099Z","iopub.status.idle":"2025-05-25T19:03:26.899059Z","shell.execute_reply.started":"2025-05-25T19:00:44.931072Z","shell.execute_reply":"2025-05-25T19:03:26.898498Z"}},"outputs":[{"name":"stderr","text":"2025-05-25 19:00:49.071682: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748199649.521017      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748199649.649270      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ca6eaea4a8a4009aa93964975629ae3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d193759975964d7b8f420eb2dad25b7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45c5a915072d4c648bb6f67eefc02193"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf8f358a735f4c3a9bf27a06ed6980c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c83e74df71534e56b86a274c24c736df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b942190d5a104a59a0420bfe6561acbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2903b177402d443d8e0b3155aac9b630"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a0c576cb26941bcac447c38a4a81979"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8972b3df2124d7e87baff3b53092358"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00f26e47858a40158f2d92d26854b97c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"538eabf218254e35a7e6448a5b9410bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/460 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8e5fca1340246099c108d2a232a69de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.bin:   0%|          | 0.00/12.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06febc7b0be4497b8ed131d36be0fae1"}},"metadata":{}},{"name":"stdout","text":"PEFT config: {'default': LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='base_models/Llama-2-7b-hf', revision=None, inference_mode=True, r=8, target_modules={'q_proj', 'v_proj', 'k_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# *ZSP*","metadata":{}},{"cell_type":"code","source":"# # Cell 4: Load and prep dataset\n# dataset = load_dataset(\"ChanceFocus/flare-finqa\", split=\"test[:200]\")\n\n# def build_prompt(example):\n#     return (\n#         \"Please answer the given financial question based on the context.\\n\"\n#         f\"Question: {example.get('input', '')}\\n\"\n#         \"Answer:\"\n#     )\n\n# import re\n# def extract_closest_number(answer, ground_truth):\n#     numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", answer)\n#     if not numbers:\n#         return answer.strip()\n#     try:\n#         gt = float(ground_truth.replace(',', ''))\n#         best_num = min(numbers, key=lambda x: abs(float(x) - gt))\n#         return best_num\n#     except Exception:\n#         return numbers[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:03:26.900502Z","iopub.execute_input":"2025-05-25T19:03:26.901317Z","iopub.status.idle":"2025-05-25T19:03:29.165136Z","shell.execute_reply.started":"2025-05-25T19:03:26.901297Z","shell.execute_reply":"2025-05-25T19:03:29.164604Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc6bd3d80c88404d80b47587398e77a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-76a97cdb03ed8a9c.parquet:   0%|          | 0.00/12.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0fc92fb44d945f599f808b50b60bf12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-5ed0ee6b1f761c33.parquet:   0%|          | 0.00/2.18M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3a5d389707d4960bafad192e572530e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-ebe922b746bd1328.parquet:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a64387fa94f48ce93c43457f616e26e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/6251 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a774475d2cf466c847297b637bb00de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1147 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"972bf06272b94cf9bf0aa143556cf984"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating valid split:   0%|          | 0/883 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24ea3c6723cc4eafba1531f74bd860da"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# *FSP*","metadata":{}},{"cell_type":"code","source":"# from datasets import load_dataset\n\n# dataset = load_dataset(\"ChanceFocus/flare-finqa\", split=\"test[:200]\")\n\n# # Few-shot examples\n# FEW_SHOT_EXAMPLES = [\n#     {\n#         \"input\": \"If a company has a net income of $2,000,000 and total equity of $10,000,000, what is its Return on Equity (ROE)?\",\n#         \"output\": \"Return on Equity (ROE) = Net Income / Total Equity = 2,000,000 / 10,000,000 = 0.2 or 20%.\"\n#     },\n#     {\n#         \"input\": \"A firm has assets worth $50,000 and liabilities worth $30,000. What is the equity?\",\n#         \"output\": \"Equity = Assets - Liabilities = 50,000 - 30,000 = 20,000.\"\n#     },\n#     {\n#         \"input\": \"If the revenue is $150,000 and expenses are $90,000, what is the profit?\",\n#         \"output\": \"Profit = Revenue - Expenses = 150,000 - 90,000 = 60,000.\"\n#     }\n# ]\n\n# def build_prompt(example):\n#     prompt = \"Please answer the given financial question based on the context.\\n\"\n#     for few_shot in FEW_SHOT_EXAMPLES:\n#         prompt += f\"Question: {few_shot['input']}\\nAnswer: {few_shot['output']}\\n\\n\"\n#     prompt += f\"Question: {example.get('input', '')}\\nAnswer:\"\n#     return prompt\n\n# import re\n# def extract_closest_number(answer, ground_truth):\n#     numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", answer)\n#     if not numbers:\n#         return answer.strip()\n#     try:\n#         gt = float(ground_truth.replace(',', ''))\n#         best_num = min(numbers, key=lambda x: abs(float(x) - gt))\n#         return best_num\n#     except Exception:\n#         return numbers[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:20:28.718248Z","iopub.execute_input":"2025-05-25T19:20:28.718558Z","iopub.status.idle":"2025-05-25T19:20:29.928080Z","shell.execute_reply.started":"2025-05-25T19:20:28.718539Z","shell.execute_reply":"2025-05-25T19:20:29.927538Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# *COTP*","metadata":{}},{"cell_type":"code","source":"# from datasets import load_dataset\n\n# dataset = load_dataset(\"ChanceFocus/flare-finqa\", split=\"test[:200]\")\n\n# # Chain-of-thought prompt\n# def build_prompt(example):\n#     return (\n#         \"Please answer the given financial question by reasoning step by step before giving your final answer.\\n\"\n#         f\"Question: {example.get('input', '')}\\n\"\n#         \"Let's think step by step.\\n\"\n#         \"Answer:\"\n#     )\n\n# import re\n# def extract_closest_number(answer, ground_truth):\n#     numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", answer)\n#     if not numbers:\n#         return answer.strip()\n#     try:\n#         gt = float(ground_truth.replace(',', ''))\n#         best_num = min(numbers, key=lambda x: abs(float(x) - gt))\n#         return best_num\n#     except Exception:\n#         return numbers[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:27:54.158584Z","iopub.execute_input":"2025-05-25T19:27:54.159183Z","iopub.status.idle":"2025-05-25T19:27:55.250733Z","shell.execute_reply.started":"2025-05-25T19:27:54.159161Z","shell.execute_reply":"2025-05-25T19:27:55.250028Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# *MP*","metadata":{}},{"cell_type":"code","source":"# from datasets import load_dataset\n\n# dataset = load_dataset(\"ChanceFocus/flare-finqa\", split=\"test[:200]\")\n\n# # Meta-prompting: explicitly instructing the model to select and apply the best reasoning strategy for the question.\n# def build_prompt(example):\n#     return (\n#         \"You are a helpful financial reasoning assistant. Given a financial question, first decide which reasoning strategy is best suited to answer it (e.g., arithmetic, comparison, logical deduction, step-by-step calculation, etc.). \"\n#         \"Briefly mention your chosen strategy, then apply it to answer the question. Show your reasoning before giving the final answer.\\n\"\n#         f\"Question: {example.get('input', '')}\\n\"\n#         \"Strategy:\\n\"\n#         \"Reasoning:\\n\"\n#         \"Answer:\"\n#     )\n\n# import re\n# def extract_closest_number(answer, ground_truth):\n#     numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", answer)\n#     if not numbers:\n#         return answer.strip()\n#     try:\n#         gt = float(ground_truth.replace(',', ''))\n#         best_num = min(numbers, key=lambda x: abs(float(x) - gt))\n#         return best_num\n#     except Exception:\n#         return numbers[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:35:59.104666Z","iopub.execute_input":"2025-05-25T19:35:59.105409Z","iopub.status.idle":"2025-05-25T19:36:00.229335Z","shell.execute_reply.started":"2025-05-25T19:35:59.105387Z","shell.execute_reply":"2025-05-25T19:36:00.228800Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# *GKP*","metadata":{}},{"cell_type":"code","source":"# from datasets import load_dataset\n\n# dataset = load_dataset(\"ChanceFocus/flare-finqa\", split=\"test[:200]\")\n\n# # Generated knowledge prompting: ask the model to generate or recall any relevant financial knowledge or formulas before answering.\n# def build_prompt(example):\n#     return (\n#         \"You are a financial expert. Before answering, first recall or generate any relevant financial knowledge, facts, or formulas that might help with the question. \"\n#         \"Write out these facts or formulas, then apply them step by step to answer the question.\\n\"\n#         f\"Question: {example.get('input', '')}\\n\"\n#         \"Relevant Knowledge:\\n\"\n#         \"Reasoning:\\n\"\n#         \"Answer:\"\n#     )\n\n# import re\n# def extract_closest_number(answer, ground_truth):\n#     numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", answer)\n#     if not numbers:\n#         return answer.strip()\n#     try:\n#         gt = float(ground_truth.replace(',', ''))\n#         best_num = min(numbers, key=lambda x: abs(float(x) - gt))\n#         return best_num\n#     except Exception:\n#         return numbers[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:42:31.007628Z","iopub.execute_input":"2025-05-25T19:42:31.008353Z","iopub.status.idle":"2025-05-25T19:42:32.094848Z","shell.execute_reply.started":"2025-05-25T19:42:31.008330Z","shell.execute_reply":"2025-05-25T19:42:32.094110Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# *PC*","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"ChanceFocus/flare-finqa\", split=\"test[:200]\")\n\n# Prompt chaining: first generate hints, facts, or intermediate steps, then use them to answer the question.\ndef build_prompt(example):\n    return (\n        \"Step 1: Read the question and generate useful hints, facts, or intermediate steps that will help in solving it.\\n\"\n        f\"Question: {example.get('input', '')}\\n\"\n        \"Hints/Steps:\\n\"\n        \"Step 2: Use the above hints or steps to reason through and answer the question.\\n\"\n        \"Answer:\"\n    )\n\nimport re\ndef extract_closest_number(answer, ground_truth):\n    numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", answer)\n    if not numbers:\n        return answer.strip()\n    try:\n        gt = float(ground_truth.replace(',', ''))\n        best_num = min(numbers, key=lambda x: abs(float(x) - gt))\n        return best_num\n    except Exception:\n        return numbers[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:47:10.424485Z","iopub.execute_input":"2025-05-25T19:47:10.424785Z","iopub.status.idle":"2025-05-25T19:47:11.152249Z","shell.execute_reply.started":"2025-05-25T19:47:10.424741Z","shell.execute_reply":"2025-05-25T19:47:11.151621Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Cell 5: Build prompts, run inference, and extract answers\n# Get the first 200 samples\ndataset_200 = dataset.select(range(50))\n\n# Build prompts and targets\nprompts_200 = []\nfor ex in dataset_200:\n    question = ex['query'].strip()\n    prompts_200.append(f\"{question}\\nAnswer with a single number or 'yes'/'no' only:\")\ntargets_200 = [ex[\"answer\"] for ex in dataset_200]\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nbatch_size = 4\nout_texts_200 = []\ntotal_steps = len(prompts_200) // batch_size + int(len(prompts_200) % batch_size != 0)\n\nwith torch.no_grad():\n    for i in range(total_steps):\n        batch_prompts = prompts_200[i * batch_size:(i + 1) * batch_size]\n        if not batch_prompts:\n            continue\n        tokens = tokenizer(\n           batch_prompts,\n            return_tensors='pt',\n            padding=True,\n            max_length=768,\n            truncation=True,\n        )\n        tokens = {k: v.to(model.device) for k, v in tokens.items()}\n        outputs = model.generate(\n            **tokens,\n            max_new_tokens=34,\n            eos_token_id=tokenizer.eos_token_id,\n            temperature=0.7,\n            num_beams=2\n        )\n        decoded_outputs = [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n        for prompt, decoded, target in zip(batch_prompts, decoded_outputs, targets_200[i * batch_size:(i + 1) * batch_size]):\n            answer = decoded[len(prompt):].strip() if decoded.startswith(prompt) else decoded.strip()\n            extracted = extract_closest_number(answer, target)\n            out_texts_200.append(extracted)\n            print(f\"Ground Truth: {target}\")\n            print(f\"Extracted Answer: {extracted}\")\n           # print(\"-\" * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:47:14.471209Z","iopub.execute_input":"2025-05-25T19:47:14.471481Z","iopub.status.idle":"2025-05-25T19:49:19.415449Z","shell.execute_reply.started":"2025-05-25T19:47:14.471461Z","shell.execute_reply":"2025-05-25T19:49:19.414696Z"}},"outputs":[{"name":"stdout","text":"Ground Truth: 94.0\nExtracted Answer: 95\nGround Truth: 0.14464\nExtracted Answer: 1\nGround Truth: 0.09864\nExtracted Answer: 1\nGround Truth: 0.02899\nExtracted Answer: 0.01\nGround Truth: 1.1197\nExtracted Answer: \nGround Truth: 0.06757\nExtracted Answer: 1\nGround Truth: 0.10039\nExtracted Answer: 0.625\nGround Truth: 0.11689\nExtracted Answer: 1.13\nGround Truth: yes\nExtracted Answer: 2011\nGround Truth: -35.0\nExtracted Answer: 1\nGround Truth: -0.04365\nExtracted Answer: 10\nGround Truth: 0.22429\nExtracted Answer: \nGround Truth: 0.24566\nExtracted Answer: 0.2\nGround Truth: no\nExtracted Answer: 2\nGround Truth: 0.01714\nExtracted Answer: 5\nGround Truth: 3297.66667\nExtracted Answer: \nGround Truth: 65.0\nExtracted Answer: 50\nGround Truth: 705.25\nExtracted Answer: \nGround Truth: 0.10964\nExtracted Answer: 1\nGround Truth: 68.9\nExtracted Answer: 31\nGround Truth: -0.08422\nExtracted Answer: 1\nGround Truth: 1.1363\nExtracted Answer: \nGround Truth: 0.09302\nExtracted Answer: 5\nGround Truth: 0.01741\nExtracted Answer: 0.33\nGround Truth: 1.59172\nExtracted Answer: 30\nGround Truth: 2.63855\nExtracted Answer: 3\nGround Truth: 0.34807\nExtracted Answer: 0.4\nGround Truth: -0.30851\nExtracted Answer: 1\nGround Truth: 5.8\nExtracted Answer: 5.8\nGround Truth: 1.13599\nExtracted Answer: 2.52\nGround Truth: 0.16417\nExtracted Answer: 1\nGround Truth: 0.28238\nExtracted Answer: 5\nGround Truth: 829.0\nExtracted Answer: 829\nGround Truth: 0.05543\nExtracted Answer: 0.51\nGround Truth: 61545.0\nExtracted Answer: 61912\nGround Truth: 112.0\nExtracted Answer: 56\nGround Truth: 0.64961\nExtracted Answer: 1\nGround Truth: 0.75799\nExtracted Answer: 1.5\nGround Truth: 12.1\nExtracted Answer: 16\nGround Truth: 3876875.0\nExtracted Answer: 1939734\nGround Truth: -191.0\nExtracted Answer: 2\nGround Truth: 0.1216\nExtracted Answer: \nGround Truth: 9294571.92\nExtracted Answer: 428328\nGround Truth: 0.45103\nExtracted Answer: 1.0\nGround Truth: 892.3\nExtracted Answer: 54\nGround Truth: 31.32382\nExtracted Answer: \nGround Truth: 0.42889\nExtracted Answer: 1.0\nGround Truth: 0.07951\nExtracted Answer: 1\nGround Truth: 16.93811\nExtracted Answer: 20\nGround Truth: 0.3254\nExtracted Answer: 3\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Cell 6: Evaluate results and print F1 scores\ndef is_number(x):\n    try:\n        float(x.replace(',', ''))\n        return True\n    except Exception:\n        return False\n\ny_true, y_pred = [], []\nfor t, p in zip(targets_200, out_texts_200):\n    if is_number(t) and is_number(p):\n        y_true.append(float(t.replace(',', '')))\n        y_pred.append(float(p.replace(',', '')))\n\nfrom sklearn.metrics import f1_score, mean_absolute_error\n\n# Convert to string for exact match F1\ny_true_str = [str(x) for x in y_true]\ny_pred_str = [str(x) for x in y_pred]\n\nf1_weighted = f1_score(y_true_str, y_pred_str, average='weighted', zero_division=0)\nf1_macro = f1_score(y_true_str, y_pred_str, average='macro', zero_division=0)\nf1_micro = f1_score(y_true_str, y_pred_str, average='micro', zero_division=0)\nmae = mean_absolute_error(y_true, y_pred) if y_true and y_pred else None\n\nprint(f\"F1 Score (weighted): {f1_weighted:.4f}\")\nprint(f\"F1 Score (macro): {f1_macro:.4f}\")\nprint(f\"F1 Score (micro): {f1_micro:.4f}\")\nif mae is not None:\n  print(f\"Mean Absolute Error: {mae:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:49:24.169990Z","iopub.execute_input":"2025-05-25T19:49:24.170275Z","iopub.status.idle":"2025-05-25T19:49:24.183937Z","shell.execute_reply.started":"2025-05-25T19:49:24.170257Z","shell.execute_reply":"2025-05-25T19:49:24.183332Z"}},"outputs":[{"name":"stdout","text":"F1 Score (weighted): 0.0488\nF1 Score (macro): 0.0303\nF1 Score (micro): 0.0488\nMean Absolute Error: 263536.7298\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}